CNN Style transfering with own example
The orignal work of style transfering with deep learning by Convolutional Neural Network (CNN) is of Gatys et al (2015). There are, to my knowledge, two versions of implementation running outside. The first is from TensorFlow Tutorial, and the second is from Coursera. (For the second version, acknowledgement to access is to Mr. Kruitbosch for the Eurostat ESTP course.)

The second version is more concise. However, it is not so intuitive and more difficult to implement. In this example, attempt is made to combine these two implementations.

[1]
3 s
import os
import tensorflow
import IPython.display as display

import matplotlib.pyplot as plt
import matplotlib as mpl
mpl.rcParams['figure.figsize'] = (12, 12)
mpl.rcParams['axes.grid'] = False

import numpy as np
import PIL.Image

import scipy.io
import scipy.misc
from skimage.io import imread, imsave

# scipy.misc.imsave = imsave
from skimage.transform import resize as imresize
 
from matplotlib.pyplot import imshow
from PIL import Image
  
import tensorflow.compat.v1 as tf

%matplotlib inline

#tf.compat.v1.disable_eager_execution()
[2]
3 s
# From RUG University, load the utility functions.
import sys
try:
  from download import download
except ImportError:
  !pip3 install download
  from download import download

# download(f'https://datascience.web.rug.nl/nst_utils.py', f'nst_utils.py')
Collecting download
  Downloading download-0.3.5-py3-none-any.whl (8.8 kB)
Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from download) (2.23.0)
Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from download) (4.64.0)
Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from download) (1.15.0)
Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->download) (2.10)
Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->download) (1.24.3)
Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->download) (3.0.4)
Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->download) (2021.10.8)
Installing collected packages: download
Successfully installed download-0.3.5
Load the content and style images
Load own content image and the style image of Monet. Change the default image sizes to the new ones.

[ ]
content_image = imread("drive/MyDrive/Proj1Art/Yingfu Porträtt.JPG")
[ ]
# Change the default image height and width.
new_height = 400
new_width = int(new_height*0.618)
[ ]
content_image=imresize(content_image, (new_height,new_width,3))
imshow(content_image)

[ ]
#content_image[0]
[ ]
# download the style image of Monet from RUG Uni. 
# download(f'https://datascience.web.rug.nl/monet.jpg', f'monet.jpg')
# or 'kandinsky5.jpg' from GoogleAPIs.
# download(f'https://storage.googleapis.com/download.tensorflow.org/example_images/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg', f'kandinsky5.jpg')

# or Munch Scream (Skrik)
download(f'https://upload.wikimedia.org/wikipedia/commons/c/c5/Edvard_Munch%2C_1893%2C_The_Scream%2C_oil%2C_tempera_and_pastel_on_cardboard%2C_91_x_73_cm%2C_National_Gallery_of_Norway.jpg', f'scream.jpg') 
 

[ ]
#style_image=imresize(imread("kandinsky5.jpg"), (new_height, new_width,3))
#style_image=imresize(imread("monet.jpg"), (new_height, new_width,3))
style_image=imresize(imread("scream.jpg"), (new_height, new_width,3))

imshow(style_image)

Note that the purpose is to use the Style of the style image and apply it to the content image, not simply the style image. Otherwise, one may only take average of these two images and create a new one.

[ ]
mixed_image= (content_image+style_image)/2
[ ]
scipy.misc.imsave("mixed_image.jpg", mixed_image.astype('uint8'))
/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: mixed_image.jpg is a low contrast image
  """Entry point for launching an IPython kernel.
Compute the cost functions
The algorithm is, for the target image, to minimize the divergence to the content image as well to the "Style" from the style image (not style image self). Please see the tutorial at TensorFlow for details of the cost functions (https://www.tensorflow.org/tutorials/generative/style_transfer).

Because we need to decied which layers to characterize the content resp. style images, we have to load the VGG19 model at this stage and check the activations from the models.

First, define a cost function for the content image.

[ ]
def compute_content_cost(a_C, a_G):
    """
    Computes the content cost
    
    Arguments:
    a_C -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image C 
    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image G
    
    Returns: 
    J_content -- scalar that you compute using equation 1 above.
    """

    # Retrieve dimensions from a_G (≈1 line)
    m, n_H, n_W, n_C = a_G.get_shape().as_list()
    # m, n_H, n_W, n_C = list(a_G.shape)
    # Reshape a_C and a_G (≈2 lines)
    a_C_unrolled = tf.reshape(a_C, shape=(m, n_H*n_W, n_C))
    a_G_unrolled = tf.reshape(a_G, shape=(m, n_H*n_W, n_C))
    
    # compute the cost with tensorflow (≈1 line)
    J_content = tf.math.reduce_sum((a_C_unrolled-a_G_unrolled)**2)/(4*n_H*n_W*n_C)
    # Test not to have the standardized term /(4*n_H*n_W*n_C)
    
    return J_content
The content cost: an example.

[ ]
# Test this cost function
tf.reset_default_graph()

with tf.Session() as test:
    tf.set_random_seed(1)
    a_C = tf.random_normal([1, 50, 30, 512], mean=1, stddev=4)
    a_G = tf.random_normal([1, 50, 30, 512], mean=1, stddev=4)
    J_content = compute_content_cost(a_C, a_G)
    print("J_content = " + str(J_content.eval()))
    J_content_0 = compute_content_cost(a_C, a_C)
    print("Content cost with itself: " + str(J_content_0.eval()))
J_content = 8.013965
Content cost with itself: 0.0
Next, define a style cost. Please see the reference for details.

[ ]
def gram_matrix(A):
    """
    Argument:
    A -- matrix of shape (n_C, n_H*n_W)
    
    Returns:
    GA -- Gram matrix of A, of shape (n_C, n_C)
    """
    GA = tf.linalg.matmul(A, A, transpose_b=True)

    return GA
[ ]
def compute_layer_style_cost(a_S, a_G):
    """
    Arguments:
    a_S -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image S 
    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image G
    
    Returns: 
    J_style_layer -- tensor representing a scalar value, style cost 
    """

    # Retrieve dimensions from a_G 
    m, n_H, n_W, n_C = list(a_G.shape)
    
    # Reshape the images to have them of shape (n_C, n_H*n_W)  
    a_S_unrolled = tf.reshape(a_S, shape=[n_C, n_H*n_W])
    a_G_unrolled = tf.reshape(a_G, shape=[n_C, n_H*n_W])

    # Computing gram_matrices for both images S and G  
    GS = gram_matrix(a_S_unrolled)
    GG = gram_matrix(a_G_unrolled)

    # Computing the loss  
    J_style_layer = tf.math.reduce_sum((GS-GG)**2)/(4*n_C**2*n_H**2*n_W**2)
    # Test not to have the standardized term /(4*n_C**2*n_H**2*n_W**2)
    
    return J_style_layer
[ ]
# the size of the standarized term
print(4*(new_height*new_width)**2)
39045760000
The style cost of one layer: an example

[ ]
# Test the style cost function
tf.reset_default_graph()

with tf.Session() as test:
    tf.set_random_seed(1)
    a_S = tf.random_normal([1, 25, 15, 512], mean=1, stddev=4)
    a_G = tf.random_normal([1, 25, 15, 512], mean=1, stddev=4)
    J_style_layer = compute_layer_style_cost(a_S, a_G)
    
    print("J_style_layer = " + str(J_style_layer.eval()))
J_style_layer = 0.38549328
[ ]
print(8.01/(0.38549*2+0.005927+0.09511+0.023873374)*5)
44.704130284583236
The function compute_layer_style_cost calculates the style cost of one layer. It is said that a weighted average of several layers would be better. Load the pretrained vgg19 model next and choose a layer for the content image and other layers as weights.

[ ]
vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')
Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5
80142336/80134624 [==============================] - 1s 0us/step
80150528/80134624 [==============================] - 1s 0us/step
[ ]
for layer in vgg.layers:
  print(layer.name)
input_1
block1_conv1
block1_conv2
block1_pool
block2_conv1
block2_conv2
block2_pool
block3_conv1
block3_conv2
block3_conv3
block3_conv4
block3_pool
block4_conv1
block4_conv2
block4_conv3
block4_conv4
block4_pool
block5_conv1
block5_conv2
block5_conv3
block5_conv4
block5_pool
Suppose we choose the activations from content_layer to characterize the content image, and neighbor layers for weighting the style cost.

[ ]
content_layer = ['block4_conv2'] 

style_layers_weights = [('block5_conv3', 0.2),
                ('block2_conv1', 0.2),
                ('block3_conv1', 0.20), 
                ('block4_conv1', 0.2),
                ('block5_conv1',0.2)]
[ ]
style_layers, style_weights =map(list, zip(*style_layers_weights))
print("layers for style: ", style_layers) 
print("and weights are", style_weights)
type(style_layers)
assert sum(style_weights) == 1, "Wrong. The sum of weights should be 1." 
layers for style:  ['block5_conv3', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']
and weights are [0.2, 0.2, 0.2, 0.2, 0.2]
[ ]
def compute_style_cost(style_output, generated_output, weights=style_weights):
    """
    Computes the overall style cost from several chosen layers
    
    Arguments:
    style_output -- output layers (ndarray) for the style 
    generated_output -- output layers (ndarray) for the generated image
    weights -- weights for weighting the style cost. Note that they should be in 
              same order as the style layers.
    
    Returns: 
    J_style -- tensor representing a scalar value

    """
    
    # initialize the overall style cost
    J_style = 0

    # Set a_S to be the hidden layer activation from the layer we have selected.

    a_S = style_output

    # Set a_G to be the output of the choosen hidden layers.
    a_G = generated_output

    for i, weight in zip(range(len(a_S)), weights):  
        # Compute style_cost for the current layer
        J_style_layer = compute_layer_style_cost(a_S[i], a_G[i])

        # Add weight * J_style_layer of this layer to overall style cost
        J_style += weight * J_style_layer

    return J_style
Use the vgg19 model to create the outputs from required layers.

[ ]
def vgg_layers_output(layer_names):
  """ Creates a vgg model that returns a list of intermediate output values."""
  vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')
  vgg.trainable = False

  outputs = [vgg.get_layer(layer).output for layer in layer_names]

  model = tf.keras.Model([vgg.input], outputs)
  return model
[ ]
style_extractor = vgg_layers_output(style_layers+content_layer)
style_outputs = style_extractor(tf.reshape(style_image, (1, new_height, new_width, 3)))

[ ]
# style_outputs[0].numpy()
[ ]
# Look at the statistics of each layer's output

for name, output in zip(style_layers+content_layer, style_outputs):
  print(f'{name:20s}')
  print("  shape: ", output.shape)
  print("  min: ", tf.reduce_min(output.numpy()))
  print("  max: ", tf.math.reduce_max(output))
  print("  mean: ", tf.math.reduce_mean(output))
  print()
block5_conv3        
  shape:  (1, 25, 15, 512)
  min:  tf.Tensor(0.0, shape=(), dtype=float32)
  max:  tf.Tensor(4.313628, shape=(), dtype=float32)
  mean:  tf.Tensor(0.1339277, shape=(), dtype=float32)

block2_conv1        
  shape:  (1, 200, 123, 128)
  min:  tf.Tensor(0.0, shape=(), dtype=float32)
  max:  tf.Tensor(18.25, shape=(), dtype=float32)
  mean:  tf.Tensor(1.5859953, shape=(), dtype=float32)

block3_conv1        
  shape:  (1, 100, 61, 256)
  min:  tf.Tensor(0.0, shape=(), dtype=float32)
  max:  tf.Tensor(66.071526, shape=(), dtype=float32)
  mean:  tf.Tensor(1.7006916, shape=(), dtype=float32)

block4_conv1        
  shape:  (1, 50, 30, 512)
  min:  tf.Tensor(0.0, shape=(), dtype=float32)
  max:  tf.Tensor(172.43004, shape=(), dtype=float32)
  mean:  tf.Tensor(8.76246, shape=(), dtype=float32)

block5_conv1        
  shape:  (1, 25, 15, 512)
  min:  tf.Tensor(0.0, shape=(), dtype=float32)
  max:  tf.Tensor(26.282759, shape=(), dtype=float32)
  mean:  tf.Tensor(0.54593074, shape=(), dtype=float32)

block4_conv2        
  shape:  (1, 50, 30, 512)
  min:  tf.Tensor(0.0, shape=(), dtype=float32)
  max:  tf.Tensor(206.36137, shape=(), dtype=float32)
  mean:  tf.Tensor(5.7252584, shape=(), dtype=float32)

Define the total cost function as weighted sum of content cost and style cost. The weights can be tuned later if necessary.

[ ]
content_weight = 1
style_weight = 40
[ ]
def total_cost(J_content, J_style, alpha = content_weight, beta = style_weight):
    """
    Computes the total cost function
    
    Arguments:
    J_content -- content cost coded above
    J_style -- style cost coded above
    alpha -- hyperparameter weighting the importance of the content cost
    beta -- hyperparameter weighting the importance of the style cost
    
    Returns:
    J -- total cost as defined by the formula above.
    """

    J = alpha * J_content + beta * J_style
    

    return J
Prepare the content, style, and input images
[ ]
content_image2 = np.reshape(content_image, (1, new_height, new_width, 3))
style_image2 = np.reshape(style_image, (1, new_height, new_width, 3))
[ ]
# tf.reduce_mean(content_image2)
[ ]
noise_ratio = 0.7
Create the start image (start value) in the coming optimization.

[ ]
np.random.seed(4686)
generated_image = (noise_ratio *np.random.uniform(0, 1, (1, new_height, new_width, 3)) + (1-noise_ratio) * content_image2).astype('float32')
[ ]
imshow(generated_image[0])

The activations (targets) for the content and style images:

[ ]
style_target = style_extractor(style_image2)[0:len(style_layers)]
content_target = style_extractor(content_image2)[-1]
[ ]
image = tf.Variable(generated_image)
#mixed_image2 = (np.reshape(mixed_image,(1, new_height, new_width,3))).astype('float32')
#image = tf.Variable(mixed_image2)
# Start from a saved image (if the running is interrupted)
# image = tf.Variable(image1000)
A assist function nice to have.

[ ]
def clip_0_1(image):
  return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)
Define the optimizer and training step
[ ]
optimize = tensorflow.optimizers.Adam(learning_rate=1, beta_1=0.99, epsilon=1e-1)
[ ]
@tf.function()
def train_step(image):
  with tf.GradientTape() as tape:
    outputs = style_extractor(image)
    content_cost = compute_content_cost(content_target,outputs[-1])
    style_cost = compute_style_cost(style_target, outputs[0:len(style_layers)])
    loss = total_cost(content_cost, style_cost)

  grad = tape.gradient(loss, image)
  optimize.apply_gradients([(grad, image)])
  image.assign(clip_0_1(image))
  
[ ]
train_step(image)
imshow(image[0])


Run the optimization
For the set superparameters, running 1000 epochs yields a good result. To extend to 5000 epochs doesn't improve the result significantly.

[ ]
epochs = 1001

for i in range(epochs):
    train_step(image)
    if i % 100 == 0:
        print(f"Epoch {i} ")
    if i % 500 == 0:
        imshow((image[0].numpy()*255).astype('uint8'))
        plt.show()
        scipy.misc.imsave(f"drive/MyDrive/Proj1Art/image_{i}.jpg", (image[0].numpy()*255).astype('uint8'))      


[ ]
#scipy.stats.describe(imread("drive/MyDrive/Proj1Art/image_5000.jpg")- imread("drive/MyDrive/Proj1Art/image_4500.jpg"))
Conclusion
For the Scream style, the learning process is shown as follows.

[6]
1 s
plt.rcParams['figure.figsize'] = (20, 13)

axes = plt.subplots(1, 6)[1].ravel()

for i, axis in zip(range(6), axes):
  axis.imshow(imread(f'drive/MyDrive/Proj1Art/Scream/image_{i*1000}.jpg'))
  axis.set_xticks([])
  axis.set_yticks([])
  axis.set_title(f'{i*1000} epochs')

plt.show()

For this original transfer learning algorithm, it takes time to obtain a good result. It is important to choose right parameters (content_weight, style_weight, layers for content and style). Standardizing the images before (as in the nst_utils.py) seems not work well.
